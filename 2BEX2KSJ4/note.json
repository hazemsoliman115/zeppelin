{
  "paragraphs": [
    {
      "text": "// Import statements, hdfs paths and start and stop times\r\nimport breeze.numerics.sqrt\r\nimport ca.savitestbed.monarch.storage.HbaseDriver\r\nimport com.github.nscala_time.time.Imports._\r\nimport org.apache.spark.rdd.RDD\r\nimport org.apache.spark.sql.{DataFrame, SQLContext}\r\nimport org.apache.spark.SparkContext\r\n//import org.apache.spark.SparkConf\r\nimport ca.savitestbed.monarch.fileprocessor._\r\n//import org.apache.spark.SparkContext._\r\nimport org.apache.spark.graphx.{Graph, Edge}\r\n\r\nimport com.github.nscala_time.time.Imports._\r\n\r\nval hdfsfolders \u003d \"hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/8/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/9/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/10/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/11/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/12/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/13/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/14/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/15/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/16/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/17/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/18/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/19/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/20/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/21/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/22/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/23/*\" \r\n\r\nval startTime \u003d \"2016-03-01 01:00:00\"\r\nval endTime \u003d \"2016-03-22 23:59:00\"\r\n\r\nval hdfspaths \u003d hdfsfolders.split(\",\")",
      "dateUpdated": "Mar 10, 2016 11:06:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457650529289_-1010650822",
      "id": "20160310-225529_720445762",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import breeze.numerics.sqrt\nimport ca.savitestbed.monarch.storage.HbaseDriver\nimport com.github.nscala_time.time.Imports._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{DataFrame, SQLContext}\nimport org.apache.spark.SparkContext\nimport ca.savitestbed.monarch.fileprocessor._\nimport org.apache.spark.graphx.{Graph, Edge}\nimport com.github.nscala_time.time.Imports._\nhdfsfolders: String \u003d hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/8/*,hdfs://monarch-master/user/ubuntu/mon...startTime: String \u003d 2016-03-01 01:00:00\nendTime: String \u003d 2016-03-22 23:59:00\nhdfspaths: Array[String] \u003d Array(hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/0/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/1/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/2/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/3/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/4/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/5/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/6/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/7/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/8/8/*, hdfs://monarch-ma..."
      },
      "dateCreated": "Mar 10, 2016 10:55:29 PM",
      "dateStarted": "Mar 10, 2016 11:06:20 PM",
      "dateFinished": "Mar 10, 2016 11:06:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val ridToId \u003d Utils.getVMRidToID(sqlContext)\r\nval idToRid \u003d ridToId.map(ri \u003d\u003e (ri._2.toLong, ri._1))\r\n\r\n// **** Section 1 Generate VM node (vmId, avgCPUUtil) ****\r\nval vmridAvgCPU \u003d VmCPUUtil.calculateAvgFromFile(sqlContext, hdfspaths, startTime, endTime)\r\n// (vmId, (rid,avgCPUUtil, type))\r\nval vmidAvgCPU \u003d vmridAvgCPU.join(ridToId).map(r \u003d\u003e (r._2._2, (r._1, r._2._1, \"vm\")))\r\n\r\n\r\n// **** Section 3 Generate physical server ****\r\nval rawPmnameAvgCPU \u003d PhysicalVirtualMemoryPercentage.calculateAvgFromFile(sqlContext, hdfspaths, startTime, endTime)\r\nval pmnameAvgCPU \u003d rawPmnameAvgCPU.map(p \u003d\u003e (p._1.split(\"_\").filter(s \u003d\u003e !s.equals(\"server\")).mkString(\"_\"), p._2))\r\nval pmnameAvgCPUFormatted \u003d pmnameAvgCPU.map(pm \u003d\u003e (Utils.murmurHash(pm._1, 123).longValue()*10000000, pm._1, pm._2, \"phyMachine\"))\r\n//debug\r\n//    pmnameAvgCPU.collect().foreach(println)\r\n\r\n// **** Section 4 Generate physical server to vm relation ****\r\nval vmidToPhy \u003d Utils.getVMIdToPhyServer(sqlContext)\r\nval vmidToPhyEdge \u003d vmidToPhy.map(vmpm \u003d\u003e (vmpm._1, Utils.murmurHash(vmpm._2, 123).longValue()*10000000, 0.0, \"host\"))",
      "dateUpdated": "Mar 10, 2016 11:08:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457651170962_1260076413",
      "id": "20160310-230610_1406284781",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "ridToId: org.apache.spark.rdd.RDD[(String, Long)] \u003d MapPartitionsRDD[3543] at map at Utils.scala:38\nidToRid: org.apache.spark.rdd.RDD[(Long, String)] \u003d MapPartitionsRDD[3544] at map at \u003cconsole\u003e:92\nvmridAvgCPU: org.apache.spark.rdd.RDD[(String, Double)] \u003d MapPartitionsRDD[3557] at map at VmBase.scala:112\nvmidAvgCPU: org.apache.spark.rdd.RDD[(Long, (String, Double, String))] \u003d MapPartitionsRDD[3561] at map at \u003cconsole\u003e:103\nrawPmnameAvgCPU: org.apache.spark.rdd.RDD[(String, Double)] \u003d MapPartitionsRDD[3574] at map at PhysicalBase.scala:99\npmnameAvgCPU: org.apache.spark.rdd.RDD[(String, Double)] \u003d MapPartitionsRDD[3575] at map at \u003cconsole\u003e:100\npmnameAvgCPUFormatted: org.apache.spark.rdd.RDD[(Long, String, Double, String)] \u003d MapPartitionsRDD[3576] at map at \u003cconsole\u003e:102\nvmidToPhy: org.apache.spark.rdd.RDD[(Long, String)] \u003d MapPartitionsRDD[3581] at map at Utils.scala:52\nvmidToPhyEdge: org.apache.spark.rdd.RDD[(Long, Long, Double, String)] \u003d MapPartitionsRDD[3582] at map at \u003cconsole\u003e:92\n"
      },
      "dateCreated": "Mar 10, 2016 11:06:10 PM",
      "dateStarted": "Mar 10, 2016 11:08:48 PM",
      "dateFinished": "Mar 10, 2016 11:09:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "vmridAvgCPU.take(5).foreach(println)\nvmidAvgCPU.take(5).foreach(println)\nrawPmnameAvgCPU.take(5).foreach(println)\npmnameAvgCPU.take(5).foreach(println)\npmnameAvgCPUFormatted.take(5).foreach(println)\nvmidToPhy.take(5).foreach(println)\nvmidToPhyEdge.take(5).foreach(println)",
      "dateUpdated": "Mar 10, 2016 11:48:17 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457651245652_1089522093",
      "id": "20160310-230725_1541489322",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "(b669dd33-3ea4-4baa-bd43-e9a587e42a95,8.316666666666666)\n(ca7b3b15-d162-45f1-b587-b2e866baf702,0.27779467258671303)\n(f9ece8a2-203b-4905-bbbb-a3e88e2ce835,0.7464775865345503)\n(7a57b53b-f386-4000-be24-97361d186c3c,0.0)\n(500892c6-f87e-4a43-a736-b2970f6968ba,0.02359806658017254)\n(159350,(5a8e3066-af2b-4ee5-bbb7-09f9d5def36f,0.6616956520061423,vm))\n(171960,(ca7b3b15-d162-45f1-b587-b2e866baf702,0.277794672586713,vm))\n(177365,(f9ece8a2-203b-4905-bbbb-a3e88e2ce835,0.7464775865345504,vm))\n(190952,(500892c6-f87e-4a43-a736-b2970f6968ba,0.02359806658017254,vm))\n(171873,(4195d86f-0e2f-4ef5-8ce8-45cd6aa390e1,0.3764328733568998,vm))\n(server_trc-agent-26.savitestbed.ca,35.4075313807532)\n(server_trc-agent-20.savitestbed.ca,18.479233449477352)\n(server_trc-agent-17.savitestbed.ca,25.253138075313842)\n(server_trc-agent-18.savitestbed.ca,51.705644599303106)\n(server_trc-agent-21.savitestbed.ca,57.75230125523018)\n(trc-agent-26.savitestbed.ca,35.407531380753205)\n(trc-agent-20.savitestbed.ca,18.479233449477356)\n(trc-agent-17.savitestbed.ca,25.253138075313842)\n(trc-agent-18.savitestbed.ca,51.705644599303106)\n(trc-agent-21.savitestbed.ca,57.752301255230186)\n(11823350100000000,trc-agent-26.savitestbed.ca,35.407531380753205,phyMachine)\n(8019628020000000,trc-agent-20.savitestbed.ca,18.479233449477352,phyMachine)\n(13982320640000000,trc-agent-17.savitestbed.ca,25.25313807531384,phyMachine)\n(12219571360000000,trc-agent-18.savitestbed.ca,51.705644599303106,phyMachine)\n(13501628710000000,trc-agent-21.savitestbed.ca,57.752301255230186,phyMachine)\n(31336,trc-agent-18.savitestbed.ca)\n(31337,trc-agent-22.savitestbed.ca)\n(31339,trc-agent-23.savitestbed.ca)\n(31341,trc-agent-20.savitestbed.ca)\n(88842,trc-agent-21.savitestbed.ca)\n(31336,12219571360000000,0.0,host)\n(31337,19601897320000000,0.0,host)\n(31339,12428157650000000,0.0,host)\n(31341,8019628020000000,0.0,host)\n(88842,13501628710000000,0.0,host)\n"
      },
      "dateCreated": "Mar 10, 2016 11:07:25 PM",
      "dateStarted": "Mar 10, 2016 11:48:17 PM",
      "dateFinished": "Mar 10, 2016 11:48:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\r\nfrom pylab import figure, axes, pie, title, show\r\n\r\n# Make a square figure and axes\r\nfigure(1, figsize\u003d(6, 6))\r\nax \u003d axes([0.1, 0.1, 0.8, 0.8])\r\n\r\n\r\nfracs \u003d [15, 30, 45, 10]\r\n\r\nexplode \u003d (0, 0.05, 0, 0)\r\nplot(fracs, explode\u003dexplode)\r\ntitle(\u0027Raining Hogs and Dogs\u0027, bbox\u003d{\u0027facecolor\u0027: \u00270.8\u0027, \u0027pad\u0027: 5})\r\n\r\n",
      "dateUpdated": "Mar 11, 2016 7:04:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457651808462_-1868817658",
      "id": "20160310-231648_768018485",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "pyspark\r interpreter not found"
      },
      "dateCreated": "Mar 10, 2016 11:16:48 PM",
      "dateStarted": "Mar 11, 2016 7:04:41 PM",
      "dateFinished": "Mar 11, 2016 7:04:41 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457723081379_-1752768877",
      "id": "20160311-190441_1237744809",
      "dateCreated": "Mar 11, 2016 7:04:41 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "PHY_Connectivity",
  "id": "2BEX2KSJ4",
  "angularObjects": {
    "2BCEZ2KJP": [],
    "2BD3PUZZ8": [],
    "2BBVUYEMN": [],
    "2BC6TWRV2": [],
    "2BEWMTK13": [],
    "2BEPEH11C": [],
    "2BDMSC5AD": [],
    "2BEW8QC4D": [],
    "2BF95Q92B": [],
    "2BC911G5T": [],
    "2BEU9CPXA": [],
    "2BBXKT3NQ": [],
    "2BDPHN3Z4": [],
    "2BCDQN15Y": []
  },
  "config": {},
  "info": {}
}
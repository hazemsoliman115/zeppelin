{
  "paragraphs": [
    {
      "text": "\r\n\r\nimport breeze.numerics.sqrt\r\nimport ca.savitestbed.monarch.storage.HbaseDriver\r\nimport com.github.nscala_time.time.Imports._\r\nimport org.apache.spark.graphx.{Graph, Edge}\r\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\r\nimport org.apache.spark.rdd.RDD\r\nimport org.apache.spark.sql.{DataFrame, SQLContext}\r\nimport org.apache.spark.{SparkContext, SparkConf}\r\nimport ca.savitestbed.monarch.fileprocessor._\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.mllib.clustering._\r\n",
      "dateUpdated": "Mar 12, 2016 1:21:39 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457745579871_350633110",
      "id": "20160312-011939_1069132347",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import breeze.numerics.sqrt\nimport ca.savitestbed.monarch.storage.HbaseDriver\nimport com.github.nscala_time.time.Imports._\nimport org.apache.spark.graphx.{Graph, Edge}\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{DataFrame, SQLContext}\nimport org.apache.spark.{SparkContext, SparkConf}\nimport ca.savitestbed.monarch.fileprocessor._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.mllib.clustering._\n"
      },
      "dateCreated": "Mar 12, 2016 1:19:39 AM",
      "dateStarted": "Mar 12, 2016 1:21:39 AM",
      "dateFinished": "Mar 12, 2016 1:21:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def virtualGetData(sc:SparkContext, sqlContext:SQLContext, hdfsfolders:String, startTime:String, endTime:String):RDD[((String, Long), Array[Double])]\u003d{\r\nval hdfspaths \u003d hdfsfolders.split(\",\")\r\nval rid_disctime__cpuutil \u003d VmCPUUtil.calculatePeriodAvgPerRidFromFile(sqlContext, hdfspaths, startTime, endTime)\r\n      .map(s \u003d\u003e ((s._1, s._2._1), s._2._2.toDouble)).coalesce(10)\r\nval rid_disctime__diskrb \u003d VmDiskReadByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__diskwb \u003d VmDiskWriteByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__netinbyte \u003d VmNetIncomingByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__netoutbyte \u003d VmNetOutgoingByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\n// This section joins data by resource ID to create the training data set\r\n// The result dataset is organized as:\r\n// for each descrete time (resource_id, (cpu_util, disk Read, disk Write, netIn, netOut, OFInBW, OFOutBW))\r\nval rid_dt__2:RDD[((String, Long), Array[Double])] \u003d rid_disctime__cpuutil.join(rid_disctime__diskrb).filter(s \u003d\u003e s._2._1.isInstanceOf[Double] \u0026\u0026 s._2._2.isInstanceOf[Double]).map(s \u003d\u003e (s._1, Array[Double](s._2._1, s._2._2)))\r\nval rid_dt__3:RDD[((String, Long), Array[Double])] \u003d rid_dt__2.join(rid_disctime__diskwb).map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\nval rid_dt__4:RDD[((String, Long), Array[Double])] \u003d rid_dt__3.join(rid_disctime__netinbyte).map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\nval rid_dt__5:RDD[((String, Long), Array[Double])] \u003d rid_dt__4.join(rid_disctime__netoutbyte).map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\nval resultData \u003d rid_dt__5\r\nrid_dt__5.map(x \u003d\u003e (x._1,x._2.mkString(\" \"))).saveAsTextFile(\"hdfs://monarch-master/user/ubuntu/monitoring/joseph/data11.csv\")\r\nresultData\r\n}\r\n",
      "dateUpdated": "Mar 12, 2016 2:08:26 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457745622851_-1737353198",
      "id": "20160312-012022_2137126249",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "virtualGetData: (sc: org.apache.spark.SparkContext, sqlContext: org.apache.spark.sql.SQLContext, hdfsfolders: String, startTime: String, endTime: String)org.apache.spark.rdd.RDD[((String, Long), Array[Double])]\n"
      },
      "dateCreated": "Mar 12, 2016 1:20:22 AM",
      "dateStarted": "Mar 12, 2016 2:08:26 AM",
      "dateFinished": "Mar 12, 2016 2:08:27 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val hdfs\u003d\"hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/8/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/9/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/10/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/11/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/12/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/13/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/14/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/15/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/16/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/17/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/18/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/19/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/20/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/21/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/22/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/23/*\"\nval startTime \u003d \"2016-03-07 00:00:00\"\nval endTime \u003d  \"2016-03-07 23:00:00\"",
      "dateUpdated": "Mar 12, 2016 2:07:06 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457745709153_-281308474",
      "id": "20160312-012149_1782892233",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "hdfs: String \u003d hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/11/8/*,hdfs://monarch-master/user/ubuntu/m...startTime: String \u003d 2016-03-07 00:00:00\nendTime: String \u003d 2016-03-07 23:00:00\n"
      },
      "dateCreated": "Mar 12, 2016 1:21:49 AM",
      "dateStarted": "Mar 12, 2016 2:07:06 AM",
      "dateFinished": "Mar 12, 2016 2:07:07 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "virtualGetData(sc, sqlContext, hdfs, startTime, endTime)\r\n",
      "dateUpdated": "Mar 12, 2016 2:08:29 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457745974854_2147209615",
      "id": "20160312-012614_36109407",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://monarch-master/user/ubuntu/monitoring/joseph/data1.csv already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.virtualGetData(\u003cconsole\u003e:66)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:62)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:69)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:71)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:73)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:75)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:77)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:79)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:81)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:83)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:85)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:87)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:89)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:91)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:93)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:95)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:97)\n\tat \u003cinit\u003e(\u003cconsole\u003e:99)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:103)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Mar 12, 2016 1:26:14 AM",
      "dateStarted": "Mar 12, 2016 2:07:11 AM",
      "dateFinished": "Mar 12, 2016 2:08:01 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457746158563_-2138651758",
      "id": "20160312-012918_1528393981",
      "dateCreated": "Mar 12, 2016 1:29:18 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Joseph",
  "id": "2BE2CZGWS",
  "angularObjects": {
    "2BCEZ2KJP": [],
    "2BD3PUZZ8": [],
    "2BBVUYEMN": [],
    "2BC6TWRV2": [],
    "2BEWMTK13": [],
    "2BEPEH11C": [],
    "2BDMSC5AD": [],
    "2BEW8QC4D": [],
    "2BF95Q92B": [],
    "2BC911G5T": [],
    "2BEU9CPXA": [],
    "2BBXKT3NQ": [],
    "2BDPHN3Z4": [],
    "2BCDQN15Y": []
  },
  "config": {},
  "info": {}
}
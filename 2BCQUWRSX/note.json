{
  "paragraphs": [
    {
      "text": "// Import statements, hdfs paths and start and stop times\r\nimport breeze.numerics.sqrt\r\nimport ca.savitestbed.monarch.storage.HbaseDriver\r\nimport com.github.nscala_time.time.Imports._\r\nimport org.apache.spark.rdd.RDD\r\nimport org.apache.spark.sql.{DataFrame, SQLContext}\r\nimport org.apache.spark.SparkContext\r\n//import org.apache.spark.SparkConf\r\nimport ca.savitestbed.monarch.fileprocessor._\r\n//import org.apache.spark.SparkContext._\r\nimport org.apache.spark.graphx.{Graph, Edge}\r\n\r\nval hdfsfolders \u003d \"hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/8/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/9/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/10/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/11/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/12/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/13/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/14/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/15/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/16/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/17/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/18/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/19/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/20/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/21/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/22/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/2/11/23/*\" \r\n\r\nval startTime \u003d \"2016-02-10 01:00:00\"\r\nval endTime \u003d \"2016-02-22 23:59:00\"\r\n\r\nval hdfspaths \u003d hdfsfolders.split(\",\")",
      "dateUpdated": "Mar 1, 2016 12:58:20 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793879237_-1567827309",
      "id": "20160301-005759_907325495",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import breeze.numerics.sqrt\n\u003cconsole\u003e:25: error: not found: value ca\n       import ca.savitestbed.monarch.storage.HbaseDriver\n              ^\n"
      },
      "dateCreated": "Mar 1, 2016 12:57:59 AM",
      "dateStarted": "Mar 1, 2016 12:58:20 AM",
      "dateFinished": "Mar 1, 2016 12:58:20 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// -------------------------------------------------------read the data objects till the openflow/networking part\r\nval rid_disctime__cpuutil \u003d VmCPUUtil.calculatePeriodAvgPerRidFromFile(sqlContext, hdfspaths, startTime, endTime).map(s \u003d\u003e ((s._1, s._2._1), s._2._2.toDouble)).coalesce(10)\r\nval rid_disctime__diskrb \u003d VmDiskReadByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__diskwb \u003d VmDiskWriteByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__netinbyte \u003d VmNetIncomingByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__netoutbyte \u003d VmNetOutgoingByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\n\r\n\r\n\r\n\r\n  \r\n  \r\n  \r\n\r\n  \r\n  \r\n\r\n// -------------------------------------------------------read the networking data objects\r\nval commPairDisctimeBwutil \u003d OFFlowBw.calculatePeriodAvgPerCommPairFromFile(sqlContext, hdfspaths, startTime, endTime)\r\nval macToRid \u003d Utils.getMacToVMRid(sqlContext)\r\nval ridToId \u003d Utils.getVMRidToID(sqlContext)\r\nval idToRid \u003d ridToId.map(ri \u003d\u003e (ri._2.toLong, ri._1))\r\n\r\n// (rid, (mac, id))\r\nval ridToMacId \u003d macToRid.map(mr \u003d\u003e (mr._2, mr._1)).join(ridToId)\r\nval macToId \u003d ridToMacId.map(rmi \u003d\u003e rmi._2)\r\n\r\n// produce (source Mac, ((des MAC, (discrete_time, average bandwidth)), srcVMid))\r\nval convertedSrcMac \u003d commPairDisctimeBwutil.map(p \u003d\u003e (p._1, (p._2, p._3))).join(macToId)\r\n\r\n// produce (dst Mac, ((srcVMId, (discrete_time, average bandwidth)), dstVMId))\r\nval convertedSrcDstMac \u003d convertedSrcMac.map(p \u003d\u003e (p._2._1._1, (p._2._2, p._2._1._2))).join(macToId)\r\n\r\n//((srcVMId, dstVMId, discrete_time), BW)\r\nval idToIdDisctimeBw \u003d convertedSrcDstMac.map(p \u003d\u003e ((p._2._1._1, p._2._2, p._2._1._2._1), p._2._1._2._2))\r\n  .reduceByKey((a, b) \u003d\u003e a+b)\r\n\r\n// Produce ((rid, dt), value)\r\nval sendBWDisctimeBW \u003d idToIdDisctimeBw\r\n  .map(p \u003d\u003e ((p._1._1, p._1._3), p._2))\r\n  .reduceByKey((a, b) \u003d\u003e a+b)\r\n  .map(f \u003d\u003e (f._1._1, (f._1._2, f._2)))\r\n  .join(idToRid)//(id, ((dt, value), rid))\r\n  .map(f \u003d\u003e ((f._2._2, f._2._1._1), f._2._1._2))\r\n\r\n// Produce ((rid, dt), value)\r\nval receiveBWDisctimeBW \u003d idToIdDisctimeBw\r\n  .map(p \u003d\u003e ((p._1._2, p._1._3), p._2))\r\n  .reduceByKey((a, b) \u003d\u003e a+b)\r\n  .map(f \u003d\u003e (f._1._1, (f._1._2, f._2)))\r\n  .join(idToRid)//(id, ((dt, value), rid))\r\n  .map(f \u003d\u003e ((f._2._2, f._2._1._1), f._2._1._2))\r\n  \r\n  \r\n//-------------------------------------------------join the data objects\r\nval rid_dt__2:RDD[((String, Long), Array[Double])] \u003d rid_disctime__cpuutil.join(rid_disctime__diskrb)\r\n      .filter(s \u003d\u003e s._2._1.isInstanceOf[Double] \u0026\u0026 s._2._2.isInstanceOf[Double])\r\n      .map(s \u003d\u003e (s._1, Array[Double](s._2._1, s._2._2)))\r\n\r\nval rid_dt__3:RDD[((String, Long), Array[Double])] \u003d rid_dt__2.join(rid_disctime__diskwb)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__4:RDD[((String, Long), Array[Double])] \u003d rid_dt__3.join(rid_disctime__netinbyte)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__5:RDD[((String, Long), Array[Double])] \u003d rid_dt__4.join(rid_disctime__netoutbyte)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__6:RDD[((String, Long), Array[Double])] \u003d rid_dt__5.join(receiveBWDisctimeBW)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__7:RDD[((String, Long), Array[Double])] \u003d rid_dt__6.join(sendBWDisctimeBW)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n  \r\n  \r\n  \r\n//-----------------------------------------------some processing needed for the graph generation\r\nval ridIdJoin \u003d rid_dt__7.map(s \u003d\u003e (s._1._1, (s._1._2, s._2))).join(ridToId)\r\nval idDtData \u003d ridIdJoin.map(s \u003d\u003e ((s._2._2,s._2._1._1), s._2._1._2))\r\n//(time,(Array,id))\r\nval DtDataid \u003d idDtData.map(s \u003d\u003e (s._1._2,(s._2,s._1._1)))\r\n//(time,(id,Array))\r\nval DtidData \u003d idDtData.map(s \u003d\u003e (s._1._2,(s._1._1,s._2)))\r\n//(time,((srcid,destid),BW))\r\nval DtSrcIdDstIdBW \u003d idToIdDisctimeBw.map(s \u003d\u003e (s._1._3,((s._1._1,s._1._2),s._2)))",
      "dateUpdated": "Mar 1, 2016 12:59:14 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793900394_2133872508",
      "id": "20160301-005820_2046802124",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:26: error: not found: value VmCPUUtil\n       val rid_disctime__cpuutil \u003d VmCPUUtil.calculatePeriodAvgPerRidFromFile(sqlContext, hdfspaths, startTime, endTime).map(s \u003d\u003e ((s._1, s._2._1), s._2._2.toDouble)).coalesce(10)\n                                   ^\n"
      },
      "dateCreated": "Mar 1, 2016 12:58:20 AM",
      "dateStarted": "Mar 1, 2016 12:59:14 AM",
      "dateFinished": "Mar 1, 2016 12:59:14 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// (time:long, Iterable[(id:long, data:Array[Double])])\nval verticesbytime \u003d DtidData.groupByKey()\n// (time:long, Iterbale[((srcid:long, destid:long),BW)])\nval edgesbytime \u003d DtSrcIdDstIdBW.groupByKey()\n// (time: Long, (vertices: (id: Long, data: Array[Double]), edges: ((srcid: Long, destid: Long), BW: double)))\nval graphelementsbytime \u003d verticesbytime.join(edgesbytime)",
      "dateUpdated": "Mar 1, 2016 12:59:25 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793954008_-1368603999",
      "id": "20160301-005914_2088002055",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:26: error: not found: value DtidData\n       val verticesbytime \u003d DtidData.groupByKey()\n                            ^\n"
      },
      "dateCreated": "Mar 1, 2016 12:59:14 AM",
      "dateStarted": "Mar 1, 2016 12:59:26 AM",
      "dateFinished": "Mar 1, 2016 12:59:26 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793965985_1654768182",
      "id": "20160301-005925_581217075",
      "dateCreated": "Mar 1, 2016 12:59:25 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Graph Matrix",
  "id": "2BCQUWRSX",
  "angularObjects": {
    "2BCEZ2KJP": [],
    "2BD3PUZZ8": [],
    "2BBVUYEMN": [],
    "2BC6TWRV2": [],
    "2BEWMTK13": [],
    "2BEPEH11C": [],
    "2BDMSC5AD": [],
    "2BEW8QC4D": [],
    "2BF95Q92B": [],
    "2BC911G5T": [],
    "2BEU9CPXA": [],
    "2BBXKT3NQ": [],
    "2BDPHN3Z4": [],
    "2BCDQN15Y": []
  },
  "config": {},
  "info": {}
}
{
  "paragraphs": [
    {
      "text": "// Import statements, hdfs paths and start and stop times\r\nimport breeze.numerics.sqrt\r\nimport ca.savitestbed.monarch.storage.HbaseDriver\r\nimport com.github.nscala_time.time.Imports._\r\nimport org.apache.spark.rdd.RDD\r\nimport org.apache.spark.sql.{DataFrame, SQLContext}\r\nimport org.apache.spark.SparkContext\r\n//import org.apache.spark.SparkConf\r\nimport ca.savitestbed.monarch.fileprocessor._\r\n//import org.apache.spark.SparkContext._\r\nimport org.apache.spark.graphx.{Graph, Edge}\r\n\r\nval hdfsfolders \u003d \"hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/8/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/9/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/10/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/11/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/12/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/13/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/14/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/15/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/16/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/17/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/18/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/19/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/20/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/21/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/22/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/23/*\" \r\n\r\nval startTime \u003d \"2016-02-10 01:00:00\"\r\nval endTime \u003d \"2016-02-22 23:59:00\"\r\n\r\nval hdfspaths \u003d hdfsfolders.split(\",\")",
      "dateUpdated": "Mar 4, 2016 10:17:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793879237_-1567827309",
      "id": "20160301-005759_907325495",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import breeze.numerics.sqrt\nimport ca.savitestbed.monarch.storage.HbaseDriver\nimport com.github.nscala_time.time.Imports._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{DataFrame, SQLContext}\nimport org.apache.spark.SparkContext\nimport ca.savitestbed.monarch.fileprocessor._\nimport org.apache.spark.graphx.{Graph, Edge}\nhdfsfolders: String \u003d hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/0/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/1/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/2/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/3/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/4/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/5/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/6/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/7/*,hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/8/*,hdfs://monarch-master/user/ubuntu/mon...startTime: String \u003d 2016-02-10 01:00:00\nendTime: String \u003d 2016-02-22 23:59:00\nhdfspaths: Array[String] \u003d Array(hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/0/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/1/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/2/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/3/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/4/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/5/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/6/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/7/*, hdfs://monarch-master/user/ubuntu/monitoring/parquet/METER_NAME_HERE/2016/3/2/8/*, hdfs://monarch-ma..."
      },
      "dateCreated": "Mar 1, 2016 12:57:59 AM",
      "dateStarted": "Mar 4, 2016 10:17:33 PM",
      "dateFinished": "Mar 4, 2016 10:18:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// -------------------------------------------------------read the data objects till the openflow/networking part\r\nval rid_disctime__cpuutil \u003d VmCPUUtil.calculatePeriodAvgPerRidFromFile(sqlContext, hdfspaths, startTime, endTime).map(s \u003d\u003e ((s._1, s._2._1), s._2._2.toDouble)).coalesce(10)\r\nval rid_disctime__diskrb \u003d VmDiskReadByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__diskwb \u003d VmDiskWriteByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__netinbyte \u003d VmNetIncomingByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\nval rid_disctime__netoutbyte \u003d VmNetOutgoingByte.calculateDerivativePerRidFromFile(sqlContext, hdfspaths, startTime, endTime).coalesce(10)\r\n\r\n\r\n\r\n\r\n  \r\n  \r\n  \r\n\r\n  \r\n  \r\n\r\n// -------------------------------------------------------read the networking data objects\r\nval commPairDisctimeBwutil \u003d OFFlowBw.calculatePeriodAvgPerCommPairFromFile(sqlContext, hdfspaths, startTime, endTime)\r\nval macToRid \u003d Utils.getMacToVMRid(sqlContext)\r\nval ridToId \u003d Utils.getVMRidToID(sqlContext)\r\nval idToRid \u003d ridToId.map(ri \u003d\u003e (ri._2.toLong, ri._1))\r\n\r\n// (rid, (mac, id))\r\nval ridToMacId \u003d macToRid.map(mr \u003d\u003e (mr._2, mr._1)).join(ridToId)\r\nval macToId \u003d ridToMacId.map(rmi \u003d\u003e rmi._2)\r\n\r\n// produce (source Mac, ((des MAC, (discrete_time, average bandwidth)), srcVMid))\r\nval convertedSrcMac \u003d commPairDisctimeBwutil.map(p \u003d\u003e (p._1, (p._2, p._3))).join(macToId)\r\n\r\n// produce (dst Mac, ((srcVMId, (discrete_time, average bandwidth)), dstVMId))\r\nval convertedSrcDstMac \u003d convertedSrcMac.map(p \u003d\u003e (p._2._1._1, (p._2._2, p._2._1._2))).join(macToId)\r\n\r\n//((srcVMId, dstVMId, discrete_time), BW)\r\nval idToIdDisctimeBw \u003d convertedSrcDstMac.map(p \u003d\u003e ((p._2._1._1, p._2._2, p._2._1._2._1), p._2._1._2._2))\r\n  .reduceByKey((a, b) \u003d\u003e a+b)\r\n\r\n// Produce ((rid, dt), value)\r\nval sendBWDisctimeBW \u003d idToIdDisctimeBw\r\n  .map(p \u003d\u003e ((p._1._1, p._1._3), p._2))\r\n  .reduceByKey((a, b) \u003d\u003e a+b)\r\n  .map(f \u003d\u003e (f._1._1, (f._1._2, f._2)))\r\n  .join(idToRid)//(id, ((dt, value), rid))\r\n  .map(f \u003d\u003e ((f._2._2, f._2._1._1), f._2._1._2))\r\n\r\n// Produce ((rid, dt), value)\r\nval receiveBWDisctimeBW \u003d idToIdDisctimeBw\r\n  .map(p \u003d\u003e ((p._1._2, p._1._3), p._2))\r\n  .reduceByKey((a, b) \u003d\u003e a+b)\r\n  .map(f \u003d\u003e (f._1._1, (f._1._2, f._2)))\r\n  .join(idToRid)//(id, ((dt, value), rid))\r\n  .map(f \u003d\u003e ((f._2._2, f._2._1._1), f._2._1._2))\r\n  \r\n  \r\n//-------------------------------------------------join the data objects\r\nval rid_dt__2:RDD[((String, Long), Array[Double])] \u003d rid_disctime__cpuutil.join(rid_disctime__diskrb)\r\n      .filter(s \u003d\u003e s._2._1.isInstanceOf[Double] \u0026\u0026 s._2._2.isInstanceOf[Double])\r\n      .map(s \u003d\u003e (s._1, Array[Double](s._2._1, s._2._2)))\r\n\r\nval rid_dt__3:RDD[((String, Long), Array[Double])] \u003d rid_dt__2.join(rid_disctime__diskwb)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__4:RDD[((String, Long), Array[Double])] \u003d rid_dt__3.join(rid_disctime__netinbyte)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__5:RDD[((String, Long), Array[Double])] \u003d rid_dt__4.join(rid_disctime__netoutbyte)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__6:RDD[((String, Long), Array[Double])] \u003d rid_dt__5.join(receiveBWDisctimeBW)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n\r\nval rid_dt__7:RDD[((String, Long), Array[Double])] \u003d rid_dt__6.join(sendBWDisctimeBW)\r\n  .map(s \u003d\u003e (s._1, s._2._1 :+ s._2._2))\r\n  \r\n  \r\n  \r\n//-----------------------------------------------some processing needed for the graph generation\r\nval ridIdJoin \u003d rid_dt__7.map(s \u003d\u003e (s._1._1, (s._1._2, s._2))).join(ridToId)\r\nval idDtData \u003d ridIdJoin.map(s \u003d\u003e ((s._2._2,s._2._1._1), s._2._1._2))\r\n//(time,(Array,id))\r\nval DtDataid \u003d idDtData.map(s \u003d\u003e (s._1._2,(s._2,s._1._1)))\r\n//(time,(id,Array))\r\nval DtidData \u003d idDtData.map(s \u003d\u003e (s._1._2,(s._1._1,s._2)))\r\n//(time,((srcid,destid),BW))\r\nval DtSrcIdDstIdBW \u003d idToIdDisctimeBw.map(s \u003d\u003e (s._1._3,((s._1._1,s._1._2),s._2)))",
      "dateUpdated": "Mar 4, 2016 10:18:26 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793900394_2133872508",
      "id": "20160301-005820_2046802124",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "rid_disctime__cpuutil: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d CoalescedRDD[14] at coalesce at \u003cconsole\u003e:48\nrid_disctime__diskrb: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d CoalescedRDD[34] at coalesce at \u003cconsole\u003e:47\nrid_disctime__diskwb: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d CoalescedRDD[54] at coalesce at \u003cconsole\u003e:47\nrid_disctime__netinbyte: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d CoalescedRDD[74] at coalesce at \u003cconsole\u003e:47\nrid_disctime__netoutbyte: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d CoalescedRDD[94] at coalesce at \u003cconsole\u003e:47\ncommPairDisctimeBwutil: org.apache.spark.rdd.RDD[(String, String, (Long, Double), String)] \u003d MapPartitionsRDD[108] at map at OFFlowBase.scala:185\nmacToRid: org.apache.spark.rdd.RDD[(String, String)] \u003d MapPartitionsRDD[113] at map at Utils.scala:31\nridToId: org.apache.spark.rdd.RDD[(String, Long)] \u003d MapPartitionsRDD[118] at map at Utils.scala:38\nidToRid: org.apache.spark.rdd.RDD[(Long, String)] \u003d MapPartitionsRDD[119] at map at \u003cconsole\u003e:41\nridToMacId: org.apache.spark.rdd.RDD[(String, (String, Long))] \u003d MapPartitionsRDD[123] at join at \u003cconsole\u003e:45\nmacToId: org.apache.spark.rdd.RDD[(String, Long)] \u003d MapPartitionsRDD[124] at map at \u003cconsole\u003e:45\nconvertedSrcMac: org.apache.spark.rdd.RDD[(String, ((String, (Long, Double)), Long))] \u003d MapPartitionsRDD[128] at join at \u003cconsole\u003e:59\nconvertedSrcDstMac: org.apache.spark.rdd.RDD[(String, ((Long, (Long, Double)), Long))] \u003d MapPartitionsRDD[132] at join at \u003cconsole\u003e:61\nidToIdDisctimeBw: org.apache.spark.rdd.RDD[((Long, Long, Long), Double)] \u003d ShuffledRDD[134] at reduceByKey at \u003cconsole\u003e:64\nsendBWDisctimeBW: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d MapPartitionsRDD[141] at map at \u003cconsole\u003e:72\nreceiveBWDisctimeBW: org.apache.spark.rdd.RDD[((String, Long), Double)] \u003d MapPartitionsRDD[148] at map at \u003cconsole\u003e:72\nrid_dt__2: org.apache.spark.rdd.RDD[((String, Long), Array[Double])] \u003d MapPartitionsRDD[153] at map at \u003cconsole\u003e:56\nrid_dt__3: org.apache.spark.rdd.RDD[((String, Long), Array[Double])] \u003d MapPartitionsRDD[157] at map at \u003cconsole\u003e:57\nrid_dt__4: org.apache.spark.rdd.RDD[((String, Long), Array[Double])] \u003d MapPartitionsRDD[161] at map at \u003cconsole\u003e:61\nrid_dt__5: org.apache.spark.rdd.RDD[((String, Long), Array[Double])] \u003d MapPartitionsRDD[165] at map at \u003cconsole\u003e:65\nrid_dt__6: org.apache.spark.rdd.RDD[((String, Long), Array[Double])] \u003d MapPartitionsRDD[169] at map at \u003cconsole\u003e:87\nrid_dt__7: org.apache.spark.rdd.RDD[((String, Long), Array[Double])] \u003d MapPartitionsRDD[173] at map at \u003cconsole\u003e:91\nridIdJoin: org.apache.spark.rdd.RDD[(String, ((Long, Array[Double]), Long))] \u003d MapPartitionsRDD[177] at join at \u003cconsole\u003e:95\nidDtData: org.apache.spark.rdd.RDD[((Long, Long), Array[Double])] \u003d MapPartitionsRDD[178] at map at \u003cconsole\u003e:93\nDtDataid: org.apache.spark.rdd.RDD[(Long, (Array[Double], Long))] \u003d MapPartitionsRDD[179] at map at \u003cconsole\u003e:96\nDtidData: org.apache.spark.rdd.RDD[(Long, (Long, Array[Double]))] \u003d MapPartitionsRDD[180] at map at \u003cconsole\u003e:96\nDtSrcIdDstIdBW: org.apache.spark.rdd.RDD[(Long, ((Long, Long), Double))] \u003d MapPartitionsRDD[181] at map at \u003cconsole\u003e:64\n"
      },
      "dateCreated": "Mar 1, 2016 12:58:20 AM",
      "dateStarted": "Mar 4, 2016 10:18:26 PM",
      "dateFinished": "Mar 4, 2016 10:19:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// (time:long, Iterable[(id:long, data:Array[Double])])\nval verticesbytime \u003d DtidData.groupByKey()\n// (time:long, Iterbale[((srcid:long, destid:long),BW)])\nval edgesbytime \u003d DtSrcIdDstIdBW.groupByKey()\n// (time: Long, (vertices: (id: Long, data: Array[Double]), edges: ((srcid: Long, destid: Long), BW: double)))\nval graphelementsbytime \u003d verticesbytime.join(edgesbytime)",
      "dateUpdated": "Mar 4, 2016 10:20:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793954008_-1368603999",
      "id": "20160301-005914_2088002055",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "verticesbytime: org.apache.spark.rdd.RDD[(Long, Iterable[(Long, Array[Double])])] \u003d ShuffledRDD[182] at groupByKey at \u003cconsole\u003e:98\nedgesbytime: org.apache.spark.rdd.RDD[(Long, Iterable[((Long, Long), Double)])] \u003d ShuffledRDD[183] at groupByKey at \u003cconsole\u003e:66\ngraphelementsbytime: org.apache.spark.rdd.RDD[(Long, (Iterable[(Long, Array[Double])], Iterable[((Long, Long), Double)]))] \u003d MapPartitionsRDD[186] at join at \u003cconsole\u003e:104\n"
      },
      "dateCreated": "Mar 1, 2016 12:59:14 AM",
      "dateStarted": "Mar 4, 2016 10:20:06 PM",
      "dateFinished": "Mar 4, 2016 10:20:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import breeze.linalg._\nobject MyFunctions {\n    def getLaplacEigen(s: Tuple2[Iterable[(Long, Array[Double])],Iterable[((Long, Long), Double)]]) \u003d{\n        //s match {\n        //    case (v, e) \u003d\u003e {\n        //        val ve \u003d v.toSeq\n        //        val ee \u003d e.toSeq\n                //val vertices \u003d sc.parallelize(v.toSeq)\n                //val edges \u003d sc.parallelize(e.toSeq).map(x \u003d\u003e Edge(x._1._1,x._1._2,x._2))\n                //val graphPerT:Graph[Array[Double], Double] \u003d Graph(vertices, edges)\n        //        println(s)\n        //    }\n        //}\n        val vertices \u003d s._1.toSeq\n        val edges \u003d s._2.toSeq\n        //val graphPerT:Graph[Array[Double], Double] \u003d Graph(vertices, edges)\n        \n        val Adj_A \u003d build_Adj(vertices, edges)\n        val Deg_A \u003d build_Deg(vertices, edges)\n        Deg_A\n    }\n    \n    def build_Adj(verticeseq: Seq[(Long, Array[Double])], edgeseq: Seq[((Long, Long), Double)]) \u003d {\n        var Adj_A \u003d DenseMatrix.zeros[Double](verticeseq.size,verticeseq.size)\n        for (edge \u003c- edgeseq){\n            Adj_A(verticeseq.indexOf(verticeseq.find(_._1 \u003d\u003d edge._1._1).get),verticeseq.indexOf(verticeseq.find(_._1 \u003d\u003d edge._1._2).get)) \u003d 1\n            Adj_A(verticeseq.indexOf(verticeseq.find(_._1 \u003d\u003d edge._1._2).get),verticeseq.indexOf(verticeseq.find(_._1 \u003d\u003d edge._1._1).get)) \u003d 1\n            \n        }\n        Adj_A\n    }\n    \n    def build_Deg(verticeseq: Seq[(Long, Array[Double])], edgeseq: Seq[((Long, Long), Double)]) \u003d {\n        var Deg_A \u003d DenseMatrix.zeros[Double](verticeseq.size,verticeseq.size)\n        for (v \u003c- verticeseq){\n            Deg_A(verticeseq.indexOf(v),verticeseq.indexOf(v)) \u003d edgeseq.count(x \u003d\u003e x._1._1 \u003d\u003d v._1 || x._1._2 \u003d\u003d v._1)\n        }\n        Deg_A\n    }\n    \n    def Graphtxt(s: Tuple2[Iterable[(Long, Array[Double])],Iterable[((Long, Long), Double)]])\u003d {\n        val vertices \u003d s._1.map(s \u003d\u003e (\"Vetrex\",s._1,s._2.mkString))\n        val edges \u003d s._2.map(s\u003d\u003e (\"edge\",s._1,s._2))\n        \n        (vertices, edges)\n    }\n}",
      "dateUpdated": "Mar 4, 2016 10:20:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793965985_1654768182",
      "id": "20160301-005925_581217075",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import breeze.linalg._\ndefined module MyFunctions\n"
      },
      "dateCreated": "Mar 1, 2016 12:59:25 AM",
      "dateStarted": "Mar 4, 2016 10:20:11 PM",
      "dateFinished": "Mar 4, 2016 10:20:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// (time, graph)\nval graphPerT \u003d graphelementsbytime.map(s \u003d\u003e MyFunctions.getLaplacEigen(s._2))\nval graphText \u003d graphelementsbytime.map(s \u003d\u003e (s._1, MyFunctions.Graphtxt(s._2)))",
      "dateUpdated": "Mar 4, 2016 10:20:26 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793980969_-1653323643",
      "id": "20160301-005940_1253135344",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "graphPerT: org.apache.spark.rdd.RDD[breeze.linalg.DenseMatrix[Double]] \u003d MapPartitionsRDD[187] at map at \u003cconsole\u003e:111\ngraphText: org.apache.spark.rdd.RDD[(Long, (Iterable[(String, Long, String)], Iterable[(String, (Long, Long), Double)]))] \u003d MapPartitionsRDD[188] at map at \u003cconsole\u003e:110\n"
      },
      "dateCreated": "Mar 1, 2016 12:59:40 AM",
      "dateStarted": "Mar 4, 2016 10:20:26 PM",
      "dateFinished": "Mar 4, 2016 10:20:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "graphText.take(2).foreach(println)\ngraphText.saveAsTextFile(\"hdfs://monarch-master/user/ubuntu/monitoring/Hazem/graph.txt\")",
      "dateUpdated": "Mar 4, 2016 10:20:34 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456793995781_-338221254",
      "id": "20160301-005955_1457858959",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Mar 1, 2016 12:59:55 AM",
      "dateStarted": "Mar 4, 2016 10:20:34 PM",
      "dateFinished": "Mar 4, 2016 10:22:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndef cleanfile(filename):\n    ret_dict \u003d {}\n    with open(filename) as f:\n        graph_list \u003d []\n        for line in f:\n            ## get the key \n            t_key \u003d \"\"\n            for x in line:\n                if x \u003d\u003d \"(\":\n                    pass\n                elif x \u003d\u003d \",\":\n                    break\n                else:\n                    t_key +\u003d x\n            ## list of preprocessed vertices\n            list_vert \u003d str(line).split(\"Vetrex\")\n            ## list of preprocessed edges\n            list_edges \u003d str(list_vert[-1]).split(\"edge\")\n            ## update last element in vertices\n            list_vert[-1] \u003d list_edges[0]\n            list_vert \u003d list_vert[1:]\n            list_edges \u003d list_edges[1:]\n            ## dict of vertices and edges\n            vert_dict \u003d {}\n            edge_list \u003d []\n            for v in list_vert:\n                ed_v \u003d v\n                list_chs \u003d \" ()List\"\n                for x in list_chs:\n                    ed_v \u003d ed_v.replace(x,\"\")\n                if ed_v[0] \u003d\u003d \",\":\n                    ed_v \u003d ed_v[1:]\n                if ed_v[-1] \u003d\u003d \",\":\n                    ed_v \u003d ed_v[:-1]\n                ded_v \u003d ed_v.split(\",\")\n                vkey \u003d ded_v[0]\n                vval \u003d ded_v[1].split(\".\")\n                vert_dict[vkey] \u003d vval\n            for e in list_edges:\n                ed_e \u003d e\n                list_chs \u003d \" ()\\n\"\n                for x in list_chs:\n                    ed_e \u003d ed_e.replace(x,\"\")\n                if ed_e[0] \u003d\u003d \",\":\n                    ed_e \u003d ed_e[1:]\n                if ed_e[-1] \u003d\u003d \",\":\n                    ed_e \u003d ed_e[:-1]\n                eval \u003d ed_e.split(\",\")\n                edge_list.append(eval)\n                ret_tuple \u003d (t_key,[vert_dict,edge_list])\n            graph_list.append(ret_tuple)\n    return graph_list\n            \n                \n    #print(t_key)\n    #print(list_vert[0])\n    #print(list_vert[1])\n    #print(list_vert[-1])\n    #print(list_edges[0])\n    #print(list_edges[1])\n    #print(list_edges[-1])\n    #print(\"Step 1\")\n    #print(ed_v)\n    #print(ded_v)\n    #print(vkey)\n    #print(vval)\n    #print(\"Step 2\")\n    #print(ed_e)\n    #print(eval)\n    \ndef build_graph(g_dict):\n    # function to build graph, input is a list of vert_dict and list_edge\n    G\u003dnx.DiGraph()\n    for v in g_dict[0].keys():\n        G.add_node(v)\n    for e in g_dict[1]:\n        G.add_edge(e[0],e[1])\n    return G\n    \ndef process_graph(G):\n    return nx.degree_centrality(G)",
      "dateUpdated": "Mar 4, 2016 10:27:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456794004268_888373564",
      "id": "20160301-010004_630648457",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Mar 1, 2016 1:00:04 AM",
      "dateStarted": "Mar 4, 2016 10:27:21 PM",
      "dateFinished": "Mar 4, 2016 10:27:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom __future__ import print_function\nimport networkx as nx\nfrom networkx import algorithms\nfrom networkx.algorithms import approximation as approx\n\nimport re\nfilename \u003d \"/home/ubuntu/collectedgraph.txt\"\n# list of (time, [vert_dict,edge_list])\ngraph_list \u003d cleanfile(filename)\n# Rdd of list( tupe(time, [v_dict, edge_list]) ) \ngraph_sc \u003d sc.parallelize(graph_list)\ngraph_sc_key \u003d graph_sc.sortByKey()\ngraph_build \u003d graph_sc_key.map(lambda s: (s[0],build_graph(s[1])))\ngraph_degree_connectivity \u003d graph_build.map(lambda s: (s[0],nx.node_connectivity(s[1])))\ngraph_degree_centrality \u003d graph_build.map(lambda s: (s[0],nx.degree_centrality(s[1])))\ngraph_degree_radius \u003d graph_build.map(lambda s: (s[0],nx.is_directed_acyclic_graph(s[1])))\n\n\nprint(graph_degree_radius.take(10))\n\n\n\n#graph_sc_built.foreach(print)",
      "dateUpdated": "Mar 4, 2016 10:32:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456794022955_-625982783",
      "id": "20160301-010022_1469123699",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[]\n"
      },
      "dateCreated": "Mar 1, 2016 1:00:22 AM",
      "dateStarted": "Mar 4, 2016 10:32:27 PM",
      "dateFinished": "Mar 4, 2016 10:32:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "graph_sc.count()",
      "dateUpdated": "Mar 4, 2016 10:32:59 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457130771069_-1522212495",
      "id": "20160304-223251_1266850188",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:43: error: not found: value graph_sc\n              graph_sc.count()\n              ^\n"
      },
      "dateCreated": "Mar 4, 2016 10:32:51 PM",
      "dateStarted": "Mar 4, 2016 10:32:59 PM",
      "dateFinished": "Mar 4, 2016 10:32:59 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//import org.apache.hadoop.conf.Configuration\r\n//import org.apache.hadoop.fs._\r\n\r\n//val hadoopConfig \u003d new Configuration()\r\n//hadoopConfig.addResource(new Path(\"/opt/hadoop-2.5.2/etc/hadoop/core-site.xml\"));\r\n//val hdfs \u003d FileSystem.get(new URL(\"hdfs://monarch-master:50070\"),hadoopConfig)\r\n//FileUtil.copyMerge(hdfs, new Path(\"hdfs://monarch-master/user/ubuntu/monitoring/Hazem/\"), hdfs, new Path(\"hdfs://monarch-master/user/ubuntu/monitoring/Hazem/collectedgraph.txt\"), false, hadoopConfig, null)",
      "dateUpdated": "Mar 1, 2016 1:00:47 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456794035077_442110812",
      "id": "20160301-010035_894534681",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Mar 1, 2016 1:00:35 AM",
      "dateStarted": "Mar 1, 2016 1:00:47 AM",
      "dateFinished": "Mar 1, 2016 1:00:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndf \u003d sqlc.createDataFrame(graph_degree_centrality)\n\n# Register to use in SQL\ndf.registerTempTable(\u0027graph_degree_centrality\u0027)",
      "dateUpdated": "Mar 1, 2016 1:00:55 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456794047167_1944386701",
      "id": "20160301-010047_406727844",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027graph_degree_centrality\u0027 is not defined\n"
      },
      "dateCreated": "Mar 1, 2016 1:00:47 AM",
      "dateStarted": "Mar 1, 2016 1:00:55 AM",
      "dateFinished": "Mar 1, 2016 1:00:55 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect * from graph_degree_centrality",
      "dateUpdated": "Mar 1, 2016 1:01:05 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456794055466_-1162460667",
      "id": "20160301-010055_539077724",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: no such table graph_degree_centrality; line 1 pos 14\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$7.applyOrElse(Analyzer.scala:229)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:212)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:229)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:933)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:933)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)\n\tat org.apache.spark.sql.DataFrame.\u003cinit\u003e(DataFrame.scala:131)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:138)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Mar 1, 2016 1:00:55 AM",
      "dateStarted": "Mar 1, 2016 1:01:05 AM",
      "dateFinished": "Mar 1, 2016 1:01:06 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456794065716_775534867",
      "id": "20160301-010105_1137661827",
      "dateCreated": "Mar 1, 2016 1:01:05 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Graph Matrix",
  "id": "2BCQUWRSX",
  "angularObjects": {
    "2BCEZ2KJP": [],
    "2BD3PUZZ8": [],
    "2BBVUYEMN": [],
    "2BC6TWRV2": [],
    "2BEWMTK13": [],
    "2BEPEH11C": [],
    "2BDMSC5AD": [],
    "2BEW8QC4D": [],
    "2BF95Q92B": [],
    "2BC911G5T": [],
    "2BEU9CPXA": [],
    "2BBXKT3NQ": [],
    "2BDPHN3Z4": [],
    "2BCDQN15Y": []
  },
  "config": {},
  "info": {}
}